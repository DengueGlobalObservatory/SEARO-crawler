name: SEARO dashboard crawling
on:
  workflow_dispatch:
  schedule:
    - cron: '00 08 * * *' # 08:00 am UTC every day

jobs:
  Scraping_data:
    runs-on: ubuntu-latest
    steps:
      - name: Checking out repo
        uses: actions/checkout@v3
      - name: Setting up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12.3'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install selenium
          pip install beautifulsoup4
          pip install pandas
          pip install requests
          pip install undetected_chromedriver
          pip install setuptools
          pip install webdriver_manager

      - name: Create Downloads directory
        run: mkdir -p ${{ github.workspace }}/output

      # Get the latest data and store it as a CSV
      #- name: Fetch latest data
      #  run: |-
      #    curl "https://raw.githubusercontent.com/ahyoung-lim/DengueCrawler/main/data/report_date.csv" -o data/report_date.csv

      - name: Run scraper
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python main_scraper.py  # Your main script name

      - name: Commit and push changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add .
          git diff --staged --quiet || git commit -m "Auto-update data $(date)"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Commit and Push
        run: |
         git config --global user.name "github-actions[bot]"
         git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"
         git add -A
         timestamp=$(date -u)
         git commit -am "Latest data: ${timestamp}" || exit 0
         git push


